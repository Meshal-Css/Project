{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edf0915",
   "metadata": {
    "id": "6edf0915"
   },
   "source": [
    "# Convolutional Neural Network (CNN) using Keras\n",
    "This notebook will guide you through the process of creating a CNN model using Keras. Follow the steps and fill in the code blocks as you progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19bf02",
   "metadata": {
    "id": "cf19bf02"
   },
   "source": [
    "# Dataset Card: Men vs. Women Classification\n",
    "\n",
    "## Overview\n",
    "The Men vs. Women Classification dataset contains images of men and women intended for binary image classification tasks. The goal is to classify images based on gender.\n",
    "\n",
    "- **Dataset URL:** [Men vs. Women Classification Dataset](https://www.kaggle.com/datasets/saadpd/menwomen-classification)\n",
    "- **Dataset Size:** ~845 MB\n",
    "- **Classes:** 2 (Men, Women)\n",
    "- **Image Format:** JPEG\n",
    "\n",
    "## Structure\n",
    "\n",
    "### Folders\n",
    "The dataset is organized into two main folders:\n",
    "\n",
    "- `traindata/`:\n",
    "  - `traindata/`: Contains the training images.\n",
    "    - `men/`: Contains images of men.\n",
    "    - `women/`: Contains images of women.\n",
    "\n",
    "- `testdata/`:\n",
    "  - `testdata/`: Contains the testing images.\n",
    "    - `men/`: Contains images of men.\n",
    "    - `women/`: Contains images of women.\n",
    "\n",
    "### Example Files\n",
    "Here are some example file names you might find in the dataset:\n",
    "\n",
    "- `traindata/traindata/men/000000899.jpg`\n",
    "- `traindata/traindata/women/00000001.jpg`\n",
    "- `testdata/testdata/men/00000504.jpg`\n",
    "- `testdata/testdata/women/00000002.jpg`\n",
    "\n",
    "### Image Specifications\n",
    "- **Resolution:** Varies\n",
    "- **Color:** RGB\n",
    "\n",
    "## Usage\n",
    "This dataset is ideal for practicing binary image classification using Convolutional Neural Networks (CNNs). It can be used to train a model to distinguish between images of men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25150e3",
   "metadata": {
    "id": "c25150e3"
   },
   "source": [
    "## Step 1: Import Required Libraries\n",
    "Begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa74530",
   "metadata": {
    "id": "aaa74530"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0ba8e",
   "metadata": {
    "id": "9df0ba8e"
   },
   "source": [
    "## Step 2: Load and Preprocess Data\n",
    "Load your dataset and preprocess it. This may include resizing images, normalizing pixel values, and splitting the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561c0bac",
   "metadata": {
    "id": "561c0bac"
   },
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\MSI1\\Desktop\\traindata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976c0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r\"C:\\Users\\MSI1\\Desktop\\traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7aed85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2891 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62093862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1330 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932d543",
   "metadata": {
    "id": "2932d543"
   },
   "source": [
    "## Step 3: Data Augmentation\n",
    "To prevent overfitting, augment your data using various transformations like rotation, zoom, flip, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e284f7",
   "metadata": {
    "id": "b0e284f7"
   },
   "outputs": [],
   "source": [
    " datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "   horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d214a",
   "metadata": {
    "id": "d77d214a"
   },
   "source": [
    "## Step 4: Build the CNN Model\n",
    "Define the architecture of your CNN model. Start with convolutional layers followed by pooling layers, and end with fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4177f7f",
   "metadata": {
    "id": "c4177f7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI1\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c1d25",
   "metadata": {
    "id": "066c1d25"
   },
   "source": [
    "## Step 5: Compile the Model\n",
    "Compile your model by specifying the optimizer, loss function, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b3fe4b",
   "metadata": {
    "id": "83b3fe4b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e31d",
   "metadata": {
    "id": "eec6e31d"
   },
   "source": [
    "## Step 6: Train the Model\n",
    "Train your model using the training data and validate it using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd761a43",
   "metadata": {
    "id": "cd761a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI1\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 0.9720 - loss: 0.0386 - val_accuracy: 1.0000 - val_loss: 1.1270e-23\n",
      "Epoch 2/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 2.3582e-34\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI1\\anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.3730e-28 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 4/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.3552e-21 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 6/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 9.8614e-25 - val_accuracy: 1.0000 - val_loss: 3.2061e-36\n",
      "Epoch 8/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 8.0843e-22\n",
      "Epoch 9/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.5158e-24 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 10/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.6443e-22 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 12/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.1576e-28 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4713e-21 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 14/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4835e-28 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 16/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4686e-27 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 18/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.8615e-22 - val_accuracy: 1.0000 - val_loss: 1.1091e-23\n",
      "Epoch 20/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "      epochs=epochs,\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=test_generator.n // test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e265a8",
   "metadata": {
    "id": "37e265a8"
   },
   "source": [
    "## Step 7: Evaluate the Model\n",
    "Evaluate the performance of your model using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08f53419",
   "metadata": {
    "id": "08f53419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.00\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_generator, verbose=0)\n",
    "print(f'Test loss: {loss:.2f}')\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb612a4c",
   "metadata": {
    "id": "eb612a4c"
   },
   "source": [
    "## Step 8: Save the Model\n",
    "Finally, save your trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9688067c",
   "metadata": {
    "id": "9688067c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('men_women_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cf14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
